{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution to Kaggle Problem 'Plant Seedlingds Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Name：AI-23**\n",
    "\n",
    "**Submission Date：16-01-2018**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this probelem, images of plant seedlings from twelve different species are provideed. The task is to accurately predict the species name of the images from the 'test' image directory. \n",
    "\n",
    "The main purpose of solving this problem are:\n",
    "* Using Keras Deep model like CNN to classify image file\n",
    "* Using pretrained Keras model to reduce the learning time and increasing efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN (Convolution Neural Network) is a state of the art model for image classification problem. Since in this experiment was to classify the test images into twelve species, CNN is used as the backbone of our solution. In the [first approach](#f) approach a cnn model consisting od total six convolution layers, three maxpooling layers,five dropout layers, one flatten layer and two fully connected neural network. In the [second approach](#s) a pretrained keras cnn model 'Xception' is imported and its weights are used as the initial weights of our CNN model. At first, weigths of all layers incuding Convnet layer are tuned since this gives increase in the training and validation accuracy per epoch more quickly. Later, when the training and validation accuracy increasee enough and training accuracy starts to take over the validation accuracy all the layer in the Xception model is 'freezed'. This gives better fine tuning in terms of validation accuracy as it prevents 'overfitting' and gives good accuracy in less amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"f\"></a>\n",
    "### First Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approch a homemade untrained CNN model is used to solve the problem. First necessary library files are imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os, cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten,MaxPool2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then structures are created for train and test lavel mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = []\n",
    "x_test = []\n",
    "y_train = []\n",
    "\n",
    "df_test = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "label_map = {   \"Black-grass\"               :0,\n",
    "                \"Charlock\"                  :1,\n",
    "                \"Cleavers\"                  :2,\n",
    "                \"Common Chickweed\"          :3,\n",
    "                \"Common wheat\"              :4,\n",
    "                \"Fat Hen\"                   :5,\n",
    "                \"Loose Silky-bent\"          :6,\n",
    "                \"Maize\"                     :7,\n",
    "                \"Scentless Mayweed\"         :8,\n",
    "                \"Shepherds Purse\"           :9,\n",
    "                \"Small-flowered Cranesbill\" :10,\n",
    "                \"Sugar beet\"                :11}\n",
    "\n",
    "dim = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then train data are prepared by reading the images files, resizing them and labelling them according to their directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:50<00:00,  4.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4750, 256, 256, 3)\n",
      "(4750, 12)\n"
     ]
    }
   ],
   "source": [
    "dirs = os.listdir(\"../seeddata/train/\")\n",
    "for k in tqdm(range(len(dirs))):    # Directory\n",
    "    files = os.listdir(\"../seeddata/train/{}\".format(dirs[k]))\n",
    "    for f in range(len(files)):     # Files\n",
    "        img = cv2.imread('../seeddata/train/{}/{}'.format(dirs[k], files[f]))\n",
    "        targets = np.zeros(12)\n",
    "        targets[label_map[dirs[k]]] = 1 \n",
    "        x_train.append(cv2.resize(img, (dim, dim)))\n",
    "        y_train.append(targets)\n",
    "        \n",
    "y_train = np.array(y_train, np.uint8)\n",
    "x_train = np.array(x_train, np.float32)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StratifiedShuffleSplit is used for balanced split for all classes to craete the train and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3990 for training and 760 for validation\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.16, random_state=42) # Want a balanced split for all the classes\n",
    "for train_index, test_index in sss.split(x_train, y_train):\n",
    "    print(\"Using {} for training and {} for validation\".format(len(train_index), len(test_index)))\n",
    "    x_train, x_valid = x_train[train_index], x_train[test_index]\n",
    "    y_train, y_valid = y_train[train_index], y_train[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then image augmentation, epochs, learning rate and batch size and a callback function is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator( horizontal_flip=True, \n",
    "                              vertical_flip=True)\n",
    "                                      \n",
    "weights = os.path.join('', 'weights.h5')\n",
    "\n",
    "epochs = 3\n",
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "\n",
    "callbacks = [ EarlyStopping(monitor='val_loss', patience=5, verbose=0), \n",
    "              ModelCheckpoint(weights, monitor='val_loss', save_best_only=True, verbose=0),\n",
    "              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN architechture is defined after oberving some performance over and training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the CNN model \n",
    "# my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> BatchNormalization -> Out\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (dim,dim,3)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(12, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained over the dataset in total 10 epochs in two steps, each step consisting of two steps for resource constraint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "125/124 [==============================] - 76s - loss: 1.9655 - acc: 0.3848 - val_loss: 1.8799 - val_acc: 0.4013\n",
      "Epoch 2/5\n",
      "125/124 [==============================] - 76s - loss: 1.8328 - acc: 0.4160 - val_loss: 1.5552 - val_acc: 0.4789\n",
      "Epoch 3/5\n",
      "125/124 [==============================] - 75s - loss: 1.6893 - acc: 0.4577 - val_loss: 1.5606 - val_acc: 0.5303\n",
      "Epoch 4/5\n",
      "125/124 [==============================] - 76s - loss: 1.5697 - acc: 0.4997 - val_loss: 1.5196 - val_acc: 0.5105\n",
      "Epoch 5/5\n",
      "125/124 [==============================] - 75s - loss: 1.4845 - acc: 0.5113 - val_loss: 1.5488 - val_acc: 0.5329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa3897a23c8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "# ------ TRAINING ------\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(x_train)/batch_size, \n",
    "                    validation_data=datagen.flow(x_valid, y_valid, batch_size=batch_size), \n",
    "                    validation_steps=len(x_valid)/batch_size,\n",
    "                    callbacks=callbacks,\n",
    "                    epochs=epochs, \n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the labels of test images are predicted and saved in the 'rawcnnsubmission.csv' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 794/794 [00:02<00:00, 271.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(794, 256, 256, 3)\n",
      "794/794 [==============================] - 5s     \n"
     ]
    }
   ],
   "source": [
    "for f, species in tqdm(df_test.values, miniters=100):\n",
    "    img = cv2.imread('../seeddata/test/{}'.format(f))\n",
    "    x_test.append(cv2.resize(img, (dim, dim)))\n",
    "\n",
    "x_test = np.array(x_test, np.float32)\n",
    "print(x_test.shape)\n",
    "\n",
    "if os.path.isfile(weights):\n",
    "    model.load_weights(weights)\n",
    "\n",
    "p_test = model.predict(x_test, verbose=1)\n",
    "\n",
    "preds = []\n",
    "for i in range(len(p_test)):\n",
    "    pos = np.argmax(p_test[i])\n",
    "    preds.append(list(label_map.keys())[list(label_map.values()).index(pos)])\n",
    "    \n",
    "df_test['species'] = preds\n",
    "df_test.to_csv('rawcnnsubmission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Kaggle Submission Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kaggle score (Mean F-Score) is 0.50629. (The screen shot of the score was taken later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kaggle Score](plantseed050.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s\"></a>\n",
    "### Second Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach, a fully connected top layer is mounted on pretrained keras model called 'Xception'. The initial weights are taken from the pretrained model. At first, all the layers including the Convnet layers are tuned since this gives faster increase in the accuracy. After reaching sufficient accuracy without any occurance of over-fitting the the tuning of the layers of the base madel (Xception) is turned off by freezing them to obtained more generalized and fine-tunned model. The preproccesing of the data like preparing training and test data are done as the same way of the [first approach](#f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os, cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = []\n",
    "x_test = []\n",
    "y_train = []\n",
    "\n",
    "df_test = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "label_map = {   \"Black-grass\"               :0,\n",
    "                \"Charlock\"                  :1,\n",
    "                \"Cleavers\"                  :2,\n",
    "                \"Common Chickweed\"          :3,\n",
    "                \"Common wheat\"              :4,\n",
    "                \"Fat Hen\"                   :5,\n",
    "                \"Loose Silky-bent\"          :6,\n",
    "                \"Maize\"                     :7,\n",
    "                \"Scentless Mayweed\"         :8,\n",
    "                \"Shepherds Purse\"           :9,\n",
    "                \"Small-flowered Cranesbill\" :10,\n",
    "                \"Sugar beet\"                :11}\n",
    "\n",
    "dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:49<00:00,  4.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4750, 256, 256, 3)\n",
      "(4750, 12)\n"
     ]
    }
   ],
   "source": [
    "# Preparing training data\n",
    "dirs = os.listdir(\"../seeddata/train/\")\n",
    "for k in tqdm(range(len(dirs))):    # Directory\n",
    "    files = os.listdir(\"../seeddata/train/{}\".format(dirs[k]))\n",
    "    for f in range(len(files)):     # Files\n",
    "        img = cv2.imread('../seeddata/train/{}/{}'.format(dirs[k], files[f]))\n",
    "        targets = np.zeros(12)\n",
    "        targets[label_map[dirs[k]]] = 1 \n",
    "        x_train.append(cv2.resize(img, (dim, dim)))\n",
    "        y_train.append(targets)\n",
    "        \n",
    "y_train = np.array(y_train, np.uint8)\n",
    "x_train = np.array(x_train, np.float32)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3990 for training and 760 for validation\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.16, random_state=42) \n",
    "for train_index, test_index in sss.split(x_train, y_train):\n",
    "    print(\"Using {} for training and {} for validation\".format(len(train_index), len(test_index)))\n",
    "    x_train, x_valid = x_train[train_index], x_train[test_index]\n",
    "    y_train, y_valid = y_train[train_index], y_train[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator( horizontal_flip=True, \n",
    "                              vertical_flip=True)\n",
    "                                      \n",
    "weights = os.path.join('', 'weights.h5')\n",
    "\n",
    "epochs = 5\n",
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "\n",
    "callbacks = [ EarlyStopping(monitor='val_loss', patience=5, verbose=0), \n",
    "              ModelCheckpoint(weights, monitor='val_loss', save_best_only=True, verbose=0),\n",
    "              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the pretrained 'Xception' model as the base model of the fully connected neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_model = Xception(input_shape=(dim, dim, 3), include_top=False, weights='imagenet', pooling='avg') # Average pooling reduces output dimensions\n",
    "x = base_model.output\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(12, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option for the freezing the layers of the 'Xception' model and loading weight saved best weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Freeze layers not in classifier due to loading imagenet weights\n",
    "for layer in base_model.layers:\n",
    "     layer.trainable = False\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "# Load any existing weights\n",
    "# if os.path.isfile(weights):\n",
    "#     model.load_weights(weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuning of all the layers are done in three epoch for its slow speed and over-fitting tendancy and then the convnet layers are freezed and the top neural network is tuned for five epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "125/124 [==============================] - 91s - loss: 0.0685 - acc: 0.9764 - val_loss: 0.0860 - val_acc: 0.9776\n",
      "Epoch 2/5\n",
      "125/124 [==============================] - 89s - loss: 0.0613 - acc: 0.9779 - val_loss: 0.1074 - val_acc: 0.9645\n",
      "Epoch 3/5\n",
      "125/124 [==============================] - 90s - loss: 0.0660 - acc: 0.9761 - val_loss: 0.0693 - val_acc: 0.9776\n",
      "Epoch 4/5\n",
      "125/124 [==============================] - 90s - loss: 0.0661 - acc: 0.9752 - val_loss: 0.0825 - val_acc: 0.9671\n",
      "Epoch 5/5\n",
      "125/124 [==============================] - 89s - loss: 0.0599 - acc: 0.9826 - val_loss: 0.0710 - val_acc: 0.9829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5d24d676a0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "# ------ TRAINING ------\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(x_train)/batch_size, \n",
    "                    validation_data=datagen.flow(x_valid, y_valid, batch_size=batch_size), \n",
    "                    validation_steps=len(x_valid)/batch_size,\n",
    "                    callbacks=callbacks,\n",
    "                    epochs=epochs, \n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing test data and prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 794/794 [00:02<00:00, 271.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(794, 256, 256, 3)\n",
      "794/794 [==============================] - 16s    \n"
     ]
    }
   ],
   "source": [
    "for f, species in tqdm(df_test.values, miniters=100):\n",
    "    img = cv2.imread('../seeddata/test/{}'.format(f))\n",
    "    x_test.append(cv2.resize(img, (dim, dim)))\n",
    "\n",
    "x_test = np.array(x_test, np.float32)\n",
    "print(x_test.shape)\n",
    "\n",
    "if os.path.isfile(weights):\n",
    "    model.load_weights(weights)\n",
    "\n",
    "p_test = model.predict(x_test, verbose=1)\n",
    "\n",
    "preds = []\n",
    "for i in range(len(p_test)):\n",
    "    pos = np.argmax(p_test[i])\n",
    "    preds.append(list(label_map.keys())[list(label_map.values()).index(pos)])\n",
    "    \n",
    "df_test['species'] = preds\n",
    "df_test.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Kaggle Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kaggle score (Mean F-Score) for this approach is 0.94836."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kaggle Score](preperfreeze94836.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick and betetr accuracy of this solution is obtained by importing pretrained model and tuninf all the layers at first and freeze them after getting sufficient accuracy without any occurance of over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
